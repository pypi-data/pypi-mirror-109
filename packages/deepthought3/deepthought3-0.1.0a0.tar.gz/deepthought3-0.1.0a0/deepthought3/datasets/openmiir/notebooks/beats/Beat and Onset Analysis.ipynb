{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This notebook about 40 MB of embedded output data, which should be put under version control.   \n",
    "Please make sure to clear all cell output using the Cell>All Output>Clear command from the menu befor committing changes!  \n",
    "Use Cell>Run all to reproduce the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import mir_eval\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import IPython.display\n",
    "from IPython.display import display\n",
    "\n",
    "import os, deepthought\n",
    "\n",
    "from deepthought3.datasets.openmiir.metadata import load_stimuli_metadata, save_beat_times\n",
    "\n",
    "STIMULI_VERSION = 2   # change to 1 for older stimuli version\n",
    "data_root = os.path.join(deepthought3.DATA_PATH, 'OpenMIIR')\n",
    "default_save_beat_times = False # change to True to save beat time to txt file\n",
    "\n",
    "def play_beats(y, sr, beats):\n",
    "    \n",
    "    if y is None:\n",
    "        # Sonify the beats only\n",
    "        y_beat = mir_eval.sonify.clicks(beats, sr, length=len(y))\n",
    "    else:\n",
    "        # Sonify the beats and add them to y\n",
    "        y_beat = y + mir_eval.sonify.clicks(beats, sr, length=len(y))    \n",
    "    \n",
    "    return IPython.display.Audio(data=y_beat, rate=sr)\n",
    "\n",
    "def visualize(y, sr, title=None, playback=True, beats=None):\n",
    "    \n",
    "    # show playback widget above figure\n",
    "    if playback:\n",
    "        if title is not None:\n",
    "            print title\n",
    "        \n",
    "        if beats is None:\n",
    "            display(IPython.display.Audio(data=y, rate=sr))\n",
    "        else:\n",
    "            beat_times = librosa.frames_to_time(beats, sr=sr, hop_length=64)\n",
    "            display(play_beats(y, sr, beat_times))\n",
    "    \n",
    "    # Let's make and display a mel-scaled power (energy-squared) spectrogram\n",
    "    # We use a small hop length of 64 here so that the frames line up with the beat tracker example below.\n",
    "    S = librosa.feature.melspectrogram(y, sr=sr, n_fft=2048, hop_length=64, n_mels=128)\n",
    "\n",
    "    # Convert to log scale (dB). We'll use the peak power as reference.\n",
    "    log_S = librosa.logamplitude(S, ref_power=np.max)\n",
    "\n",
    "    # Make a new figure\n",
    "    plt.figure(figsize=(12,4))\n",
    "\n",
    "    # Display the spectrogram on a mel scale\n",
    "    # sample rate and hop length parameters are used to render the time axis\n",
    "    librosa.display.specshow(log_S, sr=sr, hop_length=64, x_axis='time', y_axis='mel')\n",
    "\n",
    "    # Put a descriptive title on the plot\n",
    "    if title is not None:\n",
    "        plt.title('mel power spectrogram ({})'.format(title))\n",
    "\n",
    "    if beats is not None:\n",
    "        # Let's draw lines with a drop shadow on the beat events\n",
    "        plt.vlines(beats, 0, log_S.shape[0], colors='k', linestyles='-', linewidth=2.5)\n",
    "        plt.vlines(beats, 0, log_S.shape[0], colors='w', linestyles='-', linewidth=1.5)\n",
    "        \n",
    "    # draw a color bar\n",
    "    plt.colorbar(format='%+02.0f dB')\n",
    "\n",
    "    # Make the figure layout compact\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # This make sure the figures are plotted in place and not after text and audio\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "def _analyze_beats(audio_filepath, bpm, label=None, tightness=400, offset=0, duration=None, vy=True, vh=True, vp=True, vb=True):\n",
    "    print audio_filepath\n",
    "    # load audio file\n",
    "#     sr = 22050  # default\n",
    "    sr = 44100  # slower but gives better results for Harry Potter Theme\n",
    "    y, sr = librosa.load(audio_filepath, sr=sr, offset=offset, duration=duration)\n",
    "    \n",
    "    if label is not None:\n",
    "        print label\n",
    "    \n",
    "    if vy:\n",
    "        visualize(y, sr, 'original')\n",
    "    \n",
    "    # split into harmonic and percussive component\n",
    "    y_harmonic, y_percussive = librosa.effects.hpss(y)\n",
    "    \n",
    "    if vh:\n",
    "        visualize(y_harmonic, sr, 'harmonic component')\n",
    "    if vp:\n",
    "        visualize(y_percussive, sr, 'percussive component')\n",
    "    \n",
    "    # Now, let's run the beat tracker\n",
    "    # We'll use the percussive component for this part\n",
    "    # By default, the beat tracker will trim away any leading or trailing beats that don't appear strong enough.\n",
    "    # To disable this behavior, call beat_track() with trim=False.\n",
    "\n",
    "    tempo, beats = librosa.beat.beat_track(y=y_percussive, sr=sr, hop_length=64, trim=False, start_bpm=bpm, tightness=tightness)\n",
    "\n",
    "    # Let's re-draw the spectrogram, but this time, overlay the detected beats\n",
    "    if vb:\n",
    "        visualize(y, sr, 'with beats', beats=beats)\n",
    "\n",
    "    print 'Offset:                 %.4f s' % offset\n",
    "        \n",
    "    print 'Expected tempo:         %.2f BPM' % bpm\n",
    "    print 'Estimated tempo:        %.2f BPM' % tempo\n",
    "    print 'First 5 beat frames:   ', beats[:5]\n",
    "\n",
    "    # Frame numbers are great and all, but when do those beats occur?\n",
    "    print 'First 5 beat times:    ', librosa.frames_to_time(beats[:5], sr=sr, hop_length=64)\n",
    "    \n",
    "    return tempo, beats, librosa.frames_to_time(beats, sr=sr, hop_length=64)\n",
    "\n",
    "def get_audio_filepath(meta):\n",
    "    return os.path.join(data_root, 'audio', 'full.v{}'.format(STIMULI_VERSION), meta['audio_file'])\n",
    "\n",
    "def analyze_beats(meta, tightness=400, save=default_save_beat_times, **kwargs):\n",
    "    tempo, beat_frames, beat_times = _analyze_beats(\n",
    "        audio_filepath=get_audio_filepath(meta),\n",
    "        label=meta['label'], \n",
    "        bpm=meta['bpm'], \n",
    "        tightness=tightness, \n",
    "        offset=meta['length_of_cue'],\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    if save:\n",
    "        offset = meta['length_of_cue']\n",
    "        save_beat_times(beat_times, stimulus_id=meta['id'], offset=offset, version=STIMULI_VERSION)\n",
    "\n",
    "    return tempo, beat_frames, beat_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NOTE: this is experimental\n",
    "def analyze_onsets(meta):\n",
    "    audio_filepath=os.path.join(data_root, 'audio', 'full.v{}'.format(STIMULI_VERSION), meta['audio_file'])\n",
    "    sr = 44100  # slower but gives better results for Harry Potter Theme\n",
    "    offset=meta['length_of_cue']\n",
    "    duration=None\n",
    "    print sr\n",
    "    y, sr = librosa.load(audio_filepath, sr=sr, offset=offset, duration=duration)\n",
    "    \"\"\"\n",
    "    # Get onset times from a signal\n",
    "    onset_frames = librosa.onset.onset_detect(y=y, sr=sr, hop_length=64)\n",
    "    onset_times = librosa.frames_to_time(onset_frames, sr, hop_length=64)\n",
    "\n",
    "    # Or use a pre-computed onset envelope\n",
    "    o_env = librosa.onset.onset_strength(y, sr=sr)\n",
    "    onset_frames = librosa.onset.onset_detect(onset_envelope=o_env, sr=sr)\n",
    "    onset_times = librosa.frames_to_time(onset_frames, sr, hop_length=64)\n",
    "    \"\"\"\n",
    "    onset_frames = librosa.onset.onset_detect(y=y, sr=sr, hop_length=64)\n",
    "    print onset_frames\n",
    "    visualize(y, sr, 'with beats', beats=onset_frames)\n",
    "    \n",
    "    o_env = librosa.onset.onset_strength(y, sr=sr)\n",
    "    plt.plot(o_env)\n",
    "    onset_frames = librosa.onset.onset_detect(onset_envelope=o_env, sr=sr)\n",
    "    print onset_frames\n",
    "    visualize(y, sr, 'with beats', beats=onset_frames*7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meta = load_stimuli_metadata(data_root, version=STIMULI_VERSION)\n",
    "# print meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run this to analyze onsets for stimulus 22\n",
    "analyze_onsets(meta[22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run this to analyze onsets for stimulus 1, specify tightness\n",
    "tempo, beat_frames, beat_times = analyze_beats(meta[1], tightness=800)\n",
    "print beat_times\n",
    "print beat_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# analyze beginning of stimulus 22, \n",
    "# suppress visualization of original signal (vy) and harmonic (vh) and percussive (vp) component\n",
    "_analyze_beats(get_audio_filepath(meta[22]), \n",
    "               bpm=166, tightness=250, offset=2.182, duration=4.0, vy=False, vh=False, vp=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test different tightness settings on stimulus 22\n",
    "_analyze_beats(get_audio_filepath(meta[22]), \n",
    "               bpm=166, tightness=250, offset=0, duration=None, vy=False, vh=False, vp=False);\n",
    "_analyze_beats(get_audio_filepath(meta[22]), \n",
    "               bpm=166, tightness=400, offset=0, duration=None, vy=False, vh=False, vp=False);\n",
    "_analyze_beats(get_audio_filepath(meta[22]), \n",
    "               bpm=166, tightness=800, offset=0, duration=None, vy=False, vh=False, vp=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analyze stimuli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = analyze_beats(meta[1], tightness=1000, vy=False, vh=False, vp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = analyze_beats(meta[2], tightness=800, vy=False, vh=False, vp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = analyze_beats(meta[3], tightness=800, vy=False, vh=False, vp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = analyze_beats(meta[4], tightness=800, vy=False, vh=False, vp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = analyze_beats(meta[11], tightness=800, vy=False, vh=False, vp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = analyze_beats(meta[12], tightness=800, vy=False, vh=False, vp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = analyze_beats(meta[13], tightness=800, vy=False, vh=False, vp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = analyze_beats(meta[14], tightness=800, vy=False, vh=False, vp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = analyze_beats(meta[21], tightness=800, vy=False, vh=False, vp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = analyze_beats(meta[22], tightness=300, vy=False, vh=False, vp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = analyze_beats(meta[23], tightness=800, vy=False, vh=False, vp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = analyze_beats(meta[24], tightness=800, vy=False, vh=False, vp=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analyze cue click tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from deepthought3.datasets.openmiir.constants import STIMULUS_IDS\n",
    "\n",
    "for stimulus_id in STIMULUS_IDS:\n",
    "    tempo, beat_frames, beat_times = _analyze_beats(\n",
    "        audio_filepath=os.path.join(data_root, 'audio', 'cues.v{}'.format(STIMULI_VERSION), meta[stimulus_id]['cue_file']), \n",
    "        label=meta[stimulus_id]['label'], \n",
    "        bpm=meta[stimulus_id]['cue_bpm'], \n",
    "        tightness=10000, vy=False, vh=False, vp=False\n",
    "    )\n",
    "    \n",
    "    if default_save_beat_times:\n",
    "        save_beat_times(beat_times, stimulus_id=stimulus_id, cue=True, version=STIMULI_VERSION)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
